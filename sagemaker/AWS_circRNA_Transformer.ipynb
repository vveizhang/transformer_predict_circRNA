{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e063fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n",
    "from sagemaker.pytorch import PyTorch as PyTorchEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6495f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'bucket' (str)\n"
     ]
    }
   ],
   "source": [
    "#import sagemaker session and role, as well as S3 bucket\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket='arn:aws:s3:::circ-rna'\n",
    "%store bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "863e8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "hyperparameters={\n",
    "        \"train_data_dir\": 's3://circ-rna/data/circRNA_lncRNA_train.csv',\n",
    "        \"test_data_dir\": 's3://circ-rna/data/circRNA_lncRNA_test.csv',\n",
    "        \"vocab_dir\": 's3://circ-rna/data/vocab.csv',\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\":10,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17432863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# import libraries\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcopy\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtqdm\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m tqdm\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dataset, DataLoader, dataloader\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m metrics\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m  \u001b[34mimport\u001b[39;49;00m functional \u001b[34mas\u001b[39;49;00m F\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m  \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtqdm\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m tqdm\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mast\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m metrics\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m  f1_score\n",
      "device = torch.device(\u001b[33m'\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[37m#device = torch.device('cpu')\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwarnings\u001b[39;49;00m\n",
      "warnings.filterwarnings(\u001b[33m\"\u001b[39;49;00m\u001b[33mignore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, category=\u001b[36mUserWarning\u001b[39;49;00m)\n",
      "CUDA_LAUNCH_BLOCKING=\u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
      "\n",
      "# Define the padding function to pad all sequences to max length\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpad_sequences\u001b[39;49;00m(seqs,max_length=\u001b[34m400\u001b[39;49;00m,unk_index=\u001b[34m64\u001b[39;49;00m):\n",
      "    pad_seqs=[]\n",
      "    \u001b[34mfor\u001b[39;49;00m seq \u001b[35min\u001b[39;49;00m seqs:\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m(seq))<max_length:\n",
      "            pad_seqs.append(\u001b[36mstr\u001b[39;49;00m(seq) + \u001b[33m\"\u001b[39;49;00m\u001b[33m0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m * (max_length - \u001b[36mlen\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m(seq))))\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m(seq))>=max_length:\n",
      "            pad_seqs.append(seq[\u001b[34m0\u001b[39;49;00m:max_length])\n",
      "            \u001b[37m#     mid_index=max_length//2\u001b[39;49;00m\n",
      "            \u001b[37m#     pad_seqs.append((seq[:mid_index]+seq[(len(seq)-(max_length-mid_index)):],each[1]))\u001b[39;49;00m\n",
      "            \n",
      "    \u001b[34mreturn\u001b[39;49;00m pad_seqs\n",
      "\n",
      "# Define the function to build kmers\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mbuild_kmers\u001b[39;49;00m(sequence, ksize):\n",
      "    kmers = []    \n",
      "    n_kmers = \u001b[36mlen\u001b[39;49;00m(sequence) - ksize + \u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(n_kmers):\n",
      "        kmer = sequence[i:i + ksize]\n",
      "        kmers.append(kmer)\n",
      "    \u001b[34mreturn\u001b[39;49;00m kmers\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mKmers\u001b[39;49;00m(sequence): \n",
      "    Kmers=[]   \n",
      "    \u001b[34mfor\u001b[39;49;00m seq \u001b[35min\u001b[39;49;00m sequence:\n",
      "        Kmers.append(build_kmers(seq,\u001b[34m5\u001b[39;49;00m))\n",
      "    \u001b[34mreturn\u001b[39;49;00m Kmers\n",
      "\n",
      "# Cause AWS s3 doesn't allow user to store .json file, so i converted the vocabulary file to .csv, and convert it back\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_vocab\u001b[39;49;00m(vocab_dir):\n",
      "    vocab_csv = pd.read_csv(vocab_dir)\n",
      "    src_vocab = \u001b[36mdict\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(vocab_csv.kmer,vocab_csv.num))\n",
      "    \u001b[34mreturn\u001b[39;49;00m src_vocab\n",
      "\n",
      "# Define MyDataset from class Dataset\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMyDataset\u001b[39;49;00m(Dataset):\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, df, src_vocab):\n",
      "        \u001b[36mself\u001b[39;49;00m.df = df\n",
      "        \u001b[36mself\u001b[39;49;00m.src_vocab = src_vocab\n",
      "        \u001b[36mself\u001b[39;49;00m.seqs = df.kmers\n",
      "        \u001b[36mself\u001b[39;49;00m.label = df.label\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, idx):\n",
      "        seqs = [src_vocab[seq] \u001b[34mfor\u001b[39;49;00m seq \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.df.iloc[idx,\u001b[34m2\u001b[39;49;00m]]\n",
      "        seqs = torch.tensor(seqs, dtype=torch.int64)\n",
      "        label = \u001b[36mself\u001b[39;49;00m.df.iloc[idx,\u001b[34m1\u001b[39;49;00m]\n",
      "        \u001b[34mreturn\u001b[39;49;00m seqs, label\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.seqs)\n",
      "\n",
      "\u001b[37m#len(src_vocab) 1366\u001b[39;49;00m\n",
      "\n",
      "# Construct transformer model using nn.Module\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mTextTransformer\u001b[39;49;00m(nn.Module):\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "    \u001b[36msuper\u001b[39;49;00m(TextTransformer,\u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
      "    \u001b[36mself\u001b[39;49;00m.wordEmbeddings = nn.Embedding(\u001b[34m1366\u001b[39;49;00m,\u001b[34m396\u001b[39;49;00m)\n",
      "    \u001b[36mself\u001b[39;49;00m.positionEmbeddings = nn.Embedding(\u001b[34m396\u001b[39;49;00m,\u001b[34m20\u001b[39;49;00m)\n",
      "    \u001b[36mself\u001b[39;49;00m.transformerLayer = nn.TransformerEncoderLayer(\u001b[34m416\u001b[39;49;00m,\u001b[34m2\u001b[39;49;00m) \n",
      "    \u001b[36mself\u001b[39;49;00m.linear1 = nn.Linear(\u001b[34m416\u001b[39;49;00m,  \u001b[34m64\u001b[39;49;00m)\n",
      "    \u001b[36mself\u001b[39;49;00m.linear2 = nn.Linear(\u001b[34m64\u001b[39;49;00m,  \u001b[34m1\u001b[39;49;00m)\n",
      "    \u001b[36mself\u001b[39;49;00m.linear3 = nn.Linear(\u001b[34m396\u001b[39;49;00m,  \u001b[34m16\u001b[39;49;00m)\n",
      "    \u001b[36mself\u001b[39;49;00m.linear4 = nn.Linear(\u001b[34m16\u001b[39;49;00m,  \u001b[34m1\u001b[39;49;00m)\n",
      "    \n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,x):\n",
      "    positions = (torch.arange(\u001b[34m0\u001b[39;49;00m,\u001b[34m396\u001b[39;49;00m).reshape(\u001b[34m1\u001b[39;49;00m,\u001b[34m396\u001b[39;49;00m) + torch.zeros(x.shape[\u001b[34m0\u001b[39;49;00m],\u001b[34m396\u001b[39;49;00m)).to(device) \n",
      "    \u001b[37m# broadcasting the tensor of positions \u001b[39;49;00m\n",
      "    sentence = torch.cat((\u001b[36mself\u001b[39;49;00m.wordEmbeddings(x.long()),\u001b[36mself\u001b[39;49;00m.positionEmbeddings(positions.long())),axis=\u001b[34m2\u001b[39;49;00m)\n",
      "    attended = \u001b[36mself\u001b[39;49;00m.transformerLayer(sentence)\n",
      "    linear1 = F.relu(\u001b[36mself\u001b[39;49;00m.linear1(attended))\n",
      "    linear2 = F.relu(\u001b[36mself\u001b[39;49;00m.linear2(linear1))\n",
      "    linear2 = linear2.view(-\u001b[34m1\u001b[39;49;00m,\u001b[34m396\u001b[39;49;00m) \u001b[37m# reshaping the layer as the transformer outputs a 2d tensor (or 3d considering the batch size)\u001b[39;49;00m\n",
      "    linear3 = F.relu(\u001b[36mself\u001b[39;49;00m.linear3(linear2))\n",
      "    out = torch.sigmoid(\u001b[36mself\u001b[39;49;00m.linear4(linear3))\n",
      "    \u001b[34mreturn\u001b[39;49;00m out\n",
      "\n",
      "# save model to device\n",
      "model = TextTransformer().to(device)\n",
      "\n",
      "# define metrics of the model\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcalculateMetrics\u001b[39;49;00m(ypred,ytrue):\n",
      "  acc  = accuracy_score(ytrue,ypred)\n",
      "  f1  = f1_score(ytrue,ypred)\n",
      "  f1_average  = f1_score(ytrue,ypred,average=\u001b[33m\"\u001b[39;49;00m\u001b[33mmacro\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "  \u001b[34mreturn\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33m f1 score: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m+\u001b[36mstr\u001b[39;49;00m(\u001b[36mround\u001b[39;49;00m(f1,\u001b[34m3\u001b[39;49;00m))+\u001b[33m\"\u001b[39;49;00m\u001b[33m f1 average: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m+\u001b[36mstr\u001b[39;49;00m(\u001b[36mround\u001b[39;49;00m(f1_average,\u001b[34m3\u001b[39;49;00m))+\u001b[33m\"\u001b[39;49;00m\u001b[33m accuracy: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m+\u001b[36mstr\u001b[39;49;00m(\u001b[36mround\u001b[39;49;00m(acc,\u001b[34m3\u001b[39;49;00m))\n",
      "\n",
      "# define function to load the model data\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = myTransformer.to(device)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\n",
      "\n",
      "# define function to convert input json data to torch tensor\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(input_data, content_type= \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "    \u001b[37m#if content_type == 'text/plain':\u001b[39;49;00m\n",
      "    \u001b[36minput\u001b[39;49;00m = json.loads(input_data)\n",
      "    seq = \u001b[36minput\u001b[39;49;00m[\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    \u001b[37m#input = bytes(input)\u001b[39;49;00m\n",
      "    seq = seq.upper().replace(\u001b[33m\"\u001b[39;49;00m\u001b[33mU\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(seq) < \u001b[34m400\u001b[39;49;00m:\n",
      "        seq =  \u001b[36mstr\u001b[39;49;00m(seq) + \u001b[33m\"\u001b[39;49;00m\u001b[33m0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m * (\u001b[34m400\u001b[39;49;00m - \u001b[36mlen\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m(seq)))\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(seq) >= \u001b[34m400\u001b[39;49;00m:\n",
      "        seq =  seq[\u001b[34m0\u001b[39;49;00m:\u001b[34m400\u001b[39;49;00m] \n",
      "    kmers = build_kmers(seq,\u001b[34m5\u001b[39;49;00m)\n",
      "    src_vocab = get_vocab(\u001b[33m'\u001b[39;49;00m\u001b[33mhttps://sagemaker-us-east-2-411668307327.s3.us-east-2.amazonaws.com/circRNA/vocab.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    tokens=[src_vocab[kmer] \u001b[34mfor\u001b[39;49;00m kmer \u001b[35min\u001b[39;49;00m kmers]\n",
      "        \u001b[37m#data=torch.tensor(tokens, dtype=torch.float32)\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.tensor(tokens, dtype=torch.float32).to(device)\n",
      "\n",
      "# define the predict function to load model and make the prediction\n",
      "\u001b[37m# Perform prediction on the deserialized object, with the loaded model\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_object, model):\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():       \n",
      "        \u001b[34mreturn\u001b[39;49;00m model(input_object.unsqueeze(\u001b[34m0\u001b[39;49;00m).to(device))\n",
      "\n",
      "# define output function to give out human readable result\n",
      "\u001b[37m#Serialize the prediction result into the desired response content type\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction, accept=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "    \u001b[37m#logger.info('Serializing the generated output.')\u001b[39;49;00m\n",
      "    result = np.round(prediction.cpu().item())\n",
      "\u001b[37m#     if result == 1.0:\u001b[39;49;00m\n",
      "\u001b[37m#         response = \"Your inqury sequence IS circRNAs\"\u001b[39;49;00m\n",
      "\u001b[37m#     else:\u001b[39;49;00m\n",
      "\u001b[37m#         response = \"Your inqury sequence IS NOT circRNAs\"    \u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(result)\n",
      "\n",
      "# define function to save the model data\n",
      "\u001b[37m# save model\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    path = os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    torch.save(model.cpu().state_dict(), path)\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[37m#parser.add_argument(\"--output_folder\", type=str, default=\"\")\u001b[39;49;00m\n",
      "    \u001b[37m#parser.add_argument(\"--model_name\", type=str, default=\"\")\u001b[39;49;00m\n",
      "    \u001b[37m#parser.add_argument(\"--n_class\", type=int, default=2)\u001b[39;49;00m\n",
      "    \u001b[37m#parser.add_argument(\"--learning_rate\", type=float, default=0.0003)\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m6\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--vocab_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[37m#parser.add_argument(\"--embed_dim\", type=int, default=200)\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--seq_length\", type=int, default=396)\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--embed_pos\", type=int, default=20)\u001b[39;49;00m\n",
      "    \u001b[37m#parser.add_argument(\"--dim_model\", type=int, default=200)\u001b[39;49;00m\n",
      "    \u001b[37m#parser.add_argument(\"--drop_out\", type=float, default=0.1)\u001b[39;49;00m\n",
      "    \u001b[37m#parser.add_argument(\"--num_head\", type=int, default=8)\u001b[39;49;00m\n",
      "    \u001b[37m#parser.add_argument(\"--num_encoder\", type=int, default=6)\u001b[39;49;00m\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "    df_train = pd.read_csv(args.train_data_dir)\n",
      "    df_train=df_train[[\u001b[33m'\u001b[39;49;00m\u001b[33mseqs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]]\n",
      "    df_test = pd.read_csv(args.test_data_dir)\n",
      "    df_test=df_test[[\u001b[33m'\u001b[39;49;00m\u001b[33mseqs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]]\n",
      "    \n",
      "    src_vocab = get_vocab(args.vocab_dir)\n",
      "    pad_seqs_train = pad_sequences(df_train.seqs)\n",
      "    df_train[\u001b[33m'\u001b[39;49;00m\u001b[33mkmers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = Kmers(pad_seqs_train)\n",
      "    \u001b[37m#src_vocab_train = Vocab(df_train['kmers'])\u001b[39;49;00m\n",
      "\n",
      "    pad_seqs_test = pad_sequences(df_test.seqs)\n",
      "    df_test[\u001b[33m'\u001b[39;49;00m\u001b[33mkmers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = Kmers(pad_seqs_test)\n",
      "    \u001b[37m#src_vocab_test = Vocab(df_test['kmers'])\u001b[39;49;00m\n",
      "\n",
      "    batch_size=args.batch_size\n",
      "\n",
      "    train_data = MyDataset(df_train, src_vocab)\n",
      "    test_data = MyDataset(df_test, src_vocab)\n",
      "\n",
      "\n",
      "    train_itr= DataLoader(dataset=train_data, batch_size=batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    test_itr= DataLoader(dataset=test_data, batch_size=batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    model = myTransformer.to(device) \n",
      "    optimizer = optim.Adagrad(myTransformer.parameters(),lr = \u001b[34m0.001\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(args.epochs):\n",
      "        trainpreds = torch.tensor([])\n",
      "        traintrues = torch.tensor([])\n",
      "        \u001b[34mfor\u001b[39;49;00m  batch \u001b[35min\u001b[39;49;00m train_itr:\n",
      "            X = batch[\u001b[34m0\u001b[39;49;00m].to(device)\n",
      "            y = batch[\u001b[34m1\u001b[39;49;00m].to(torch.float).to(device)\n",
      "            myTransformer.zero_grad()\n",
      "            pred = myTransformer(X).squeeze()\n",
      "            trainpreds = torch.cat((trainpreds,pred.cpu().detach()))\n",
      "            traintrues = torch.cat((traintrues,y.cpu().detach()))\n",
      "            err = F.binary_cross_entropy(pred,y)\n",
      "            err.backward()\n",
      "            optimizer.step()\n",
      "        err = F.binary_cross_entropy(trainpreds,traintrues)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain BCE loss: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,err.item(),calculateMetrics(torch.round(trainpreds).numpy(),traintrues.numpy()))\n",
      "\n",
      "        valpreds = torch.tensor([])\n",
      "        valtrues = torch.tensor([])\n",
      "        \u001b[34mfor\u001b[39;49;00m batch \u001b[35min\u001b[39;49;00m test_itr:\n",
      "            X = batch[\u001b[34m0\u001b[39;49;00m].to(device)\n",
      "            y = batch[\u001b[34m1\u001b[39;49;00m].to(torch.float).to(device)\n",
      "            valtrues = torch.cat((valtrues,y.cpu().detach()))\n",
      "            pred = myTransformer(X).squeeze().cpu().detach()\n",
      "    \u001b[37m# print(valtrues.shape)\u001b[39;49;00m\n",
      "            valpreds = torch.cat((valpreds,pred))\n",
      "        err = F.binary_cross_entropy(valpreds,valtrues)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation BCE loss: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,err.item(),calculateMetrics(torch.round(valpreds).numpy(),valtrues.numpy()))\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "            torch.save(model.state_dict(), f)\n"
     ]
    }
   ],
   "source": [
    "# pygmentize train.py\n",
    "!pygmentize \"source_dir/train.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea3572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorchEstimator(\n",
    "    entry_point='source_dir/train.py',\n",
    "    #source_dir='s3://sagemaker-us-east-2-411668307327/circRNA/',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    volume_size=50,\n",
    "    hyperparameters=hyperparameters,\n",
    "    #metric_definitions=metric_definitions,\n",
    "    framework_version='1.7',\n",
    "    py_version='py36',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fcef31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 19:54:58 Starting - Starting the training job...\n",
      "2022-06-22 19:55:23 Starting - Preparing the instances for trainingProfilerReport-1655927698: InProgress\n",
      ".........\n",
      "2022-06-22 19:56:41 Downloading - Downloading input data...\n",
      "2022-06-22 19:57:21 Training - Downloading the training image.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-06-22 19:58:09,338 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-06-22 19:58:09,340 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-06-22 19:58:09,353 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-06-22 19:58:09,359 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-06-22 19:58:09,751 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-06-22 19:58:09,768 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-06-22 19:58:09,783 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-06-22 19:58:09,796 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 16,\n",
      "        \"epochs\": 1,\n",
      "        \"test_data_dir\": \"s3://circ-rna/data/circRNA_lncRNA_test.csv\",\n",
      "        \"train_data_dir\": \"s3://circ-rna/data/circRNA_lncRNA_train.csv\",\n",
      "        \"vocab_dir\": \"s3://circ-rna/data/vocab.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-06-22-19-54-57-452\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-411668307327/pytorch-training-2022-06-22-19-54-57-452/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m4.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":16,\"epochs\":5,\"test_data_dir\":\"s3://circ-rna/data/circRNA_lncRNA_test.csv\",\"train_data_dir\":\"s3://circ-rna/data/circRNA_lncRNA_train.csv\",\"vocab_dir\":\"s3://circ-rna/data/vocab.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-411668307327/pytorch-training-2022-06-22-19-54-57-452/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":16,\"epochs\":1,\"test_data_dir\":\"s3://circ-rna/data/circRNA_lncRNA_test.csv\",\"train_data_dir\":\"s3://circ-rna/data/circRNA_lncRNA_train.csv\",\"vocab_dir\":\"s3://circ-rna/data/vocab.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-06-22-19-54-57-452\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-411668307327/pytorch-training-2022-06-22-19-54-57-452/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"16\",\"--epochs\",\"1\",\"--test_data_dir\",\"s3://circ-rna/data/circRNA_lncRNA_test.csv\",\"--train_data_dir\",\"s3://circ-rna/data/circRNA_lncRNA_train.csv\",\"--vocab_dir\",\"s3://circ-rna/data/vocab.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_DATA_DIR=s3://circ-rna/data/circRNA_lncRNA_test.csv\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_DIR=s3://circ-rna/data/circRNA_lncRNA_train.csv\u001b[0m\n",
      "\u001b[34mSM_HP_VOCAB_DIR=s3://circ-rna/data/vocab.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --batch_size 16 --epochs 1 --test_data_dir s3://circ-rna/data/circRNA_lncRNA_test.csv --train_data_dir s3://circ-rna/data/circRNA_lncRNA_train.csv --vocab_dir s3://circ-rna/data/vocab.csv\u001b[0m\n",
      "\n",
      "2022-06-22 19:58:22 Training - Training image download completed. Training in progress.\u001b[34m[2022-06-22 19:58:21.510 algo-1:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.649 algo-1:26 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.650 algo-1:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.651 algo-1:26 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.651 algo-1:26 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.652 algo-1:26 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.665 algo-1:26 INFO hook.py:591] name:wordEmbeddings.weight count_params:540936\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.665 algo-1:26 INFO hook.py:591] name:positionEmbeddings.weight count_params:7920\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.666 algo-1:26 INFO hook.py:591] name:transformerLayer.self_attn.in_proj_weight count_params:519168\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.666 algo-1:26 INFO hook.py:591] name:transformerLayer.self_attn.in_proj_bias count_params:1248\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.666 algo-1:26 INFO hook.py:591] name:transformerLayer.self_attn.out_proj.weight count_params:173056\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.666 algo-1:26 INFO hook.py:591] name:transformerLayer.self_attn.out_proj.bias count_params:416\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.666 algo-1:26 INFO hook.py:591] name:transformerLayer.linear1.weight count_params:851968\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.666 algo-1:26 INFO hook.py:591] name:transformerLayer.linear1.bias count_params:2048\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.666 algo-1:26 INFO hook.py:591] name:transformerLayer.linear2.weight count_params:851968\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.666 algo-1:26 INFO hook.py:591] name:transformerLayer.linear2.bias count_params:416\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.666 algo-1:26 INFO hook.py:591] name:transformerLayer.norm1.weight count_params:416\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.666 algo-1:26 INFO hook.py:591] name:transformerLayer.norm1.bias count_params:416\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.667 algo-1:26 INFO hook.py:591] name:transformerLayer.norm2.weight count_params:416\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.667 algo-1:26 INFO hook.py:591] name:transformerLayer.norm2.bias count_params:416\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.667 algo-1:26 INFO hook.py:591] name:linear1.weight count_params:26624\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.667 algo-1:26 INFO hook.py:591] name:linear1.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.667 algo-1:26 INFO hook.py:591] name:linear2.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.667 algo-1:26 INFO hook.py:591] name:linear2.bias count_params:1\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.667 algo-1:26 INFO hook.py:591] name:linear3.weight count_params:6336\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.667 algo-1:26 INFO hook.py:591] name:linear3.bias count_params:16\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.667 algo-1:26 INFO hook.py:591] name:linear4.weight count_params:16\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.668 algo-1:26 INFO hook.py:591] name:linear4.bias count_params:1\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.668 algo-1:26 INFO hook.py:593] Total Trainable Params: 2983930\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.668 algo-1:26 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-06-22 19:58:21.670 algo-1:26 INFO hook.py:488] Hook is writing from the hook with pid: 26\u001b[0m\n",
      "\u001b[34mepoch1:train BCE loss:  0.91272453  f1 score: 0.781 accuracy: 0.762\u001b[0m\n",
      "\n",
      "\u001b[34mepoch1:validation BCE loss:  0.99440112  f1 score: 0.682 accuracy: 0.706\u001b[0m\n",
      "\u001b[34mepoch2:train BCE loss:  0.84872742  f1 score: 0.821 accuracy: 0.821\u001b[0m\n",
      "\n",
      "\u001b[34mepoch2:validation BCE loss:  0.88705549  f1 score: 0.783 accuracy: 0.796\u001b[0m\n",
      "\u001b[34mepoch3:train BCE loss:  0.76034012  f1 score: 0.886 accuracy: 0.862\u001b[0m\n",
      "\n",
      "\u001b[34mepoch3:validation BCE loss:  0.83401123  f1 score: 0.823 accuracy: 0.836\u001b[0m\n",
      "\u001b[34mepoch4:train BCE loss:  0.64876953  f1 score: 0.923 accuracy: 0.932\u001b[0m\n",
      "\n",
      "\u001b[34mepoch4:validation BCE loss:  0.73059401  f1 score: 0.88 accuracy: 0.862\u001b[0m\n",
      "\u001b[34mepoch5:train BCE loss:  0.46034010  f1 score: 0.961 accuracy: 0.958\u001b[0m\n",
      "\n",
      "\u001b[34mepoch5:validation BCE loss:  0.53726624  f1 score: 0.942 accuracy: 0.943\u001b[0m\n",
      "\u001b[34m2022-06-22 21:06:51,056 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-06-22 21:07:17 Completed - Training job completed\n",
      "ProfilerReport-1655927698: NoIssuesFound\n",
      "Training seconds: 4229\n",
      "Billable seconds: 4229\n"
     ]
    }
   ],
   "source": [
    "channels = {\n",
    "    'training': \"s3://circ-rna/data/circRNA_lncRNA_train.csv\",\n",
    "    'test': \"s3://circ-rna/data/circRNA_lncRNA_test.csv\"\n",
    "}\n",
    "estimator.fit(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc4ab035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-411668307327/pytorch-training-2022-06-22-19-54-57-452/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_data = estimator.model_data\n",
    "print(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf82e779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.pytorch.model.PyTorchModel at 0x7fecc0c67dc0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_model = PyTorchModel(model_data='s3://sagemaker-us-east-2-411668307327/pytorch-training-2022-06-22-19-54-57-452/output/model.tar.gz',\n",
    "                             role=role,\n",
    "                             framework_version=\"1.7\",\n",
    "                             #source_dir=\"code\",\n",
    "                             py_version=\"py36\",\n",
    "                             entry_point=\"source_dir/train.py\")\n",
    "pytorch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e7582d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = pytorch_model.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\",serializer=JSONSerializer(), deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c119565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.pytorch.model.PyTorchPredictor at 0x7fecc00701f0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0063c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "seq=\"GCATGTTGGCATTGAACATTGACGAAGCTATTACATTGCTTGAACAATTGGGACTTAGTGGCAGCTATCAATGGTGTAATACCACAGGATGGCATTCTACAAAGTGAATATGGAGGTGAGACCATACCAGGACCTGCATTTAATCCAGCAAGTCATCCAGCTTCAGCTCCTACTTCCTCTTCTTCTTCAGCGTTTCGACCTGTAATGCCATCCAGGCAGATTGTAGAAAGGCAACCTCGGATGCTGGACTTCAGGGTTGAATACAGAGACAGAAATGTTGATGTGGTACTTGAAGACACCTGTACTGTTG\"\n",
    "result = predictor.predict({\"text\": seq})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d33f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "01ea21ff6a4f3074a68e71f39f127958401068994ac911c4fa7f15f5a28521f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
